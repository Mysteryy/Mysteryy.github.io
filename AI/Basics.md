---
layout: default
title: AI Basics
---
# AI Basics

## High Level Neural Network Overview

### **1\. Feedforward Neural Networks (FNN)**

* ✅ **Good at**: Basic pattern recognition, simple classification, and regression tasks.  
* ❌ **Bad at**: Capturing sequential dependencies or spatial structures.

### **2\. Convolutional Neural Networks (CNN)**

* ✅ **Good at**: Image processing, feature extraction, and spatial hierarchies.  
* ❌ **Bad at**: Handling sequential data or long-range dependencies.

### **3\. Recurrent Neural Networks (RNN)**

* ✅ **Good at**: Processing sequential data (e.g., time series, natural language).  
* ❌ **Bad at**: Handling long sequences due to vanishing gradient issues.

### **4\. Long Short-Term Memory (LSTM)**

* ✅ **Good at**: Learning long-range dependencies in sequential data.  
* ❌ **Bad at**: Computationally expensive, slower training.

### **5\. Gated Recurrent Unit (GRU)**

* ✅ **Good at**: Similar to LSTMs but more efficient.  
* ❌ **Bad at**: Less expressive than LSTMs for complex dependencies.

### **6\. Transformers (e.g., BERT, GPT)**

* ✅ **Good at**: Processing large-scale sequential data with attention mechanisms.  
* ❌ **Bad at**: High computational cost, requires large datasets.

### **7\. Autoencoders**

* ✅ **Good at**: Dimensionality reduction, anomaly detection.  
* ❌ **Bad at**: Generating high-quality reconstructions of complex data.

### **8\. Generative Adversarial Networks (GANs)**

* ✅ **Good at**: Generating realistic synthetic data (images, text, etc.).  
* ❌ **Bad at**: Training instability, mode collapse.

### **9\. Radial Basis Function Networks (RBFN)**

* ✅ **Good at**: Function approximation, classification with smooth decision boundaries.  
* ❌ **Bad at**: Scalability, requires a lot of computation for large datasets.

### **10\. Graph Neural Networks (GNN)**

* ✅ **Good at**: Learning from graph-structured data (e.g., social networks, molecules).  
* ❌ **Bad at**: Computationally expensive, hard to scale.